{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529d96c3",
   "metadata": {},
   "source": [
    "# Cross-entropy loss function\n",
    "I'd like to implement a method to compute the loss function for a given set of training examples and coefficients.  \n",
    "The method will be defined as `def crossEntropyLoss(X_train, y_train, coef)` where  \n",
    "- `X_train` is a 2d array of input variables corresponding to the design matrix of the training set.\n",
    "- `y_train` is a 1d array containing the output values of the examples.\n",
    "- `coef` is a 1d array containing the coefficients $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5bc774",
   "metadata": {},
   "source": [
    "I'll start by defining a scalar product function to compute each $\\mathbf{w}^T\\mathbf{x}^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec8f047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scalar_product(a, b):\n",
    "    return sum(x * y for x, y in zip(a, b))\n",
    "\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "scalar_product(a, b) # 1*4 + 2*5 + 3*6 = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21569532",
   "metadata": {},
   "source": [
    "Next, for a logistic regression model, we have,\n",
    "$$\\text{logit}(p(1|\\mathbf{x}^{(i)}, \\mathbf{w})) = \\mathbf{w}^T\\mathbf{x}^{(i)}$$\n",
    "$$\\iff p(1|\\mathbf{x}^{(i)}, \\mathbf{w}) = \\frac{e^{\\mathbf{w}^T\\mathbf{x}^{(i)}}}{1 + e^{\\mathbf{w}^T\\mathbf{x}^{(i)}}} $$\n",
    "$$\\iff p(0|\\mathbf{x}^{(i)}, \\mathbf{w}) = 1 - p(1|\\mathbf{x}^{(i)}, \\mathbf{w}) = \\frac{1}{1 + e^{\\mathbf{w}^T\\mathbf{x}^{(i)}}} $$\n",
    "So I will create a sigmoid function $\\sigma(\\theta) \\coloneqq \\frac{e^\\theta}{1 + e^\\theta} $ to calculate probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9fe1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import e\n",
    "def sigmoid_function(theta):\n",
    "    return (e ** theta)/(1+ e**theta)\n",
    "\n",
    "sigmoid_function(0) # = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e7150",
   "metadata": {},
   "source": [
    "Finally, the loss function is defined as\n",
    "$$ E(\\mathbf{w}) = -\\sum_{i = 1}^{N}{\\mathcal{L}(\\mathbf{w})} = -\\sum_{i = 1}^{N}{\\ln(p_{y^{(i)}})}, $$\n",
    "where $p_{y^{(i)}} = p(y^{(i)}| \\mathbf{x}^{(i)}, \\mathbf{w})$, $y^{(i)} \\in \\{0, 1\\}$  \n",
    "This is equivalent to\n",
    "$$ E(\\mathbf{w}) = -\\sum_{i = 1}^{N}{ y^{(i)} \\ln(p(1| \\mathbf{x}^{(i)}, \\mathbf{w})) + (1-y^{(i)}) \\ln(1 - p(1| \\mathbf{x}^{(i)}, \\mathbf{w}))}$$\n",
    "I will build the function in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd07dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def crossEntropyLoss(X_train, y_train, coeff):\n",
    "    total = 0\n",
    "    N = len(X_train)\n",
    "    for i in range(N):\n",
    "        p_1 = sigmoid_function(scalar_product(X_train[i], coeff))\n",
    "        total += y_train[i] * log(p_1) + (1 - y_train[i]) * log(1-p_1)\n",
    "    return -1 * total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67159563",
   "metadata": {},
   "source": [
    "Given some training data, we can test this function. For weights $\\mathbf{w} = \\mathbf{0}$, we can see that\n",
    "$$ \\forall \\mathbf{X} = (\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots,  \\mathbf{x}^{(N)})^T, \\mathcal{y} = \\begin{bmatrix} 1 \\\\0 \\\\1 \\end{bmatrix} \\implies E(\\mathbf{0}) = -3\\ln(\\frac{1}{2}) \\approx 2.0794 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d282cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0794415416798357"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [[1, 0.1, 0.1], [1, 0.1, 0.2], [1, 0.2, 0.2]]\n",
    "y_train = [1, 0, 1]\n",
    "coeff = [0, 0, 0]\n",
    "crossEntropyLoss(X_train, y_train, coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f07f71",
   "metadata": {},
   "source": [
    "That checks out, so we can use this to go about finding a minimal value for $E(\\mathbf{w})$ by varying $\\mathbf{w}$ in the future. Here is the condensed code, and another test for weights $\\mathbf{w} = \\begin{bmatrix} 0.1 \\\\0.2 \\\\0.3 \\end{bmatrix}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb87e674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.006287642011906"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import e, log\n",
    "def crossEntropyLoss(X_train, y_train, coeff):\n",
    "    total = 0\n",
    "    N = len(X_train)\n",
    "    for i in range(N):\n",
    "        wTx = sum(x * y for x, y in zip(X_train[i], coeff))\n",
    "        p_1 = (e ** wTx)/(1+ e**wTx)\n",
    "        total += y_train[i] * log(p_1) + (1 - y_train[i]) * log(1-p_1)\n",
    "    return -1 * total\n",
    "\n",
    "X_train = [[1, 0.1, 0.1], [1, 0.1, 0.2], [1, 0.2, 0.2]]\n",
    "y_train = [1, 0, 1]\n",
    "coeff = [0.1, 0.2, 0.3]\n",
    "crossEntropyLoss(X_train, y_train, coeff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
